{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision, backend as K\nfrom sklearn.metrics import classification_report, roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\ntf.random.set_seed(42)\nmixed_precision.set_global_policy('mixed_float16')\n\nDATASET_PATH = \"/kaggle/input/cell-images-2/cell_images\"\nIMG_SIZE = (160, 160)\nBATCH_SIZE = 64\nEPOCHS = 30\n\ndatagen = ImageDataGenerator(\n    rescale=1./255,\n    validation_split=0.2,\n    horizontal_flip=True,\n    zoom_range=0.2\n)\n\ntrain_gen = datagen.flow_from_directory(\n    DATASET_PATH,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    subset='training',\n    shuffle=True\n)\n\nval_gen = datagen.flow_from_directory(\n    DATASET_PATH,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    subset='validation',\n    shuffle=False\n)\n\n\ndef wavelet_transform(x):\n    out1 = tf.image.resize(x, (80, 80))\n    out2 = tf.image.resize(x, (40, 40))\n    out3 = tf.image.resize(x, (20, 20))\n    out4 = tf.image.resize(x, (10, 10))\n    return [out1, out2, out3, out4]\n\n\ndef WaveletTransformAxisY(x):\n    even = x[:, :, ::2]\n    odd = x[:, :, 1::2]\n    L = (even + odd) / 2\n    H = (even - odd) / 2\n    return L, H\n\ndef WaveletTransformAxisX(x):\n    even = x[:, ::2, :]\n    odd = x[:, 1::2, :]\n    L = (even + odd) / 2\n    H = (even - odd) / 2\n    return L, H\n\ndef Wavelet(batch_image):\n    batch_image = K.permute_dimensions(batch_image, [0, 3, 1, 2])\n    r, g, b = batch_image[:, 0], batch_image[:, 1], batch_image[:, 2]\n\n    def level_decompose(channel):\n        L, H = WaveletTransformAxisY(channel)\n        LL, LH = WaveletTransformAxisX(L)\n        HL, HH = WaveletTransformAxisX(H)\n        return LL, LH, HL, HH\n\n    def full_decomposition(channel):\n        out1 = level_decompose(channel)\n        out2 = level_decompose(out1[0])\n        out3 = level_decompose(out2[0])\n        out4 = level_decompose(out3[0])\n        return out1 + out2 + out3 + out4\n\n    r_decom = full_decomposition(r)\n    g_decom = full_decomposition(g)\n    b_decom = full_decomposition(b)\n\n    def group_levels(decom):\n        return [\n            K.stack(decom[0:4], axis=1),\n            K.stack(decom[4:8], axis=1),\n            K.stack(decom[8:12], axis=1),\n            K.stack(decom[12:16], axis=1)\n        ]\n\n    r_levels = group_levels(r_decom)\n    g_levels = group_levels(g_decom)\n    b_levels = group_levels(b_decom)\n\n    levels = []\n    for i in range(4):\n        level = K.concatenate([r_levels[i], g_levels[i], b_levels[i]], axis=1)\n        level = K.permute_dimensions(level, [0, 2, 3, 1])\n        levels.append(level)\n    return levels\n\ndef Wavelet_out_shape(input_shapes):\n    return [\n        (None, IMG_SIZE[0]//2, IMG_SIZE[1]//2, 12),\n        (None, IMG_SIZE[0]//4, IMG_SIZE[1]//4, 12),\n        (None, IMG_SIZE[0]//8, IMG_SIZE[1]//8, 12),\n        (None, IMG_SIZE[0]//16, IMG_SIZE[1]//16, 12)\n    ]\n\n\ndef spatial_attention(input_feature):\n    avg_pool = Lambda(lambda x: tf.reduce_mean(x, axis=-1, keepdims=True))(input_feature)\n    max_pool = Lambda(lambda x: tf.reduce_max(x, axis=-1, keepdims=True))(input_feature)\n    concat = Concatenate(axis=-1)([avg_pool, max_pool])\n    attention = Conv2D(1, kernel_size=7, padding='same', activation='sigmoid', use_bias=False)(concat)\n    return Multiply()([input_feature, attention])\n\n\nclass CheckpointedTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, num_heads=4, ff_dim=128, dropout_rate=0.1):\n        super().__init__()\n        self.num_heads = num_heads\n        self.ff_dim = ff_dim\n        self.dropout_rate = dropout_rate\n        \n        self.ln1 = LayerNormalization(epsilon=1e-6)\n        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)\n        self.ln2 = LayerNormalization(epsilon=1e-6)\n        self.dense1 = Dense(ff_dim, activation='relu')\n        self.dense2 = None\n        self.dropout = Dropout(dropout_rate)\n\n    def call(self, inputs, training=None):\n        if self.dense2 is None:\n            self.dense2 = Dense(inputs.shape[-1])\n        \n        def forward_pass(x):\n            x1 = self.ln1(x)\n            attn_output = self.mha(x1, x1)\n            x2 = x + attn_output\n            x3 = self.ln2(x2)\n            ff = self.dense1(x3)\n            ff = self.dense2(ff)\n            ff = self.dropout(ff, training=training)\n            return x2 + ff\n\n        return tf.recompute_grad(forward_pass)(inputs)\n\ndef transformer_block(inputs, num_heads=4, ff_dim=128):\n    x = LayerNormalization(epsilon=1e-6)(inputs)\n    attention = MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)(x, x)\n    x = Add()([x, attention])\n    x = LayerNormalization(epsilon=1e-6)(x)\n    ff = Dense(ff_dim, activation='relu')(x)\n    ff = Dense(inputs.shape[-1])(ff)\n    x = Add()([x, ff])\n    return x\n\n\ndef build_model(use_checkpointing=False, use_full_wavelet=False):\n    inp = Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n\n    # Choose wavelet type\n    if use_full_wavelet:\n        wavelet = Lambda(Wavelet, output_shape=Wavelet_out_shape)(inp)\n    else:\n        wavelet = Lambda(wavelet_transform)(inp)\n\n    # Apply spatial attention\n    w1 = spatial_attention(wavelet[0])\n    w2 = spatial_attention(wavelet[1])\n    w3 = spatial_attention(wavelet[2])\n    w4 = spatial_attention(wavelet[3])\n\n\n    x1 = Conv2D(64, 3, padding='same')(w1)\n    x1 = BatchNormalization()(x1)\n    x1 = Activation('relu')(x1)\n    x1 = Conv2D(64, 3, strides=2, padding='same')(x1)\n    x1 = BatchNormalization()(x1)\n    x1 = Activation('relu')(x1)\n\n    xa = Conv2D(64, 3, strides=2, padding='same')(w2)\n    xa = BatchNormalization()(xa)\n    xa = Activation('relu')(xa)\n\n    x1 = MaxPooling2D(pool_size=(2, 2))(x1)\n    x = Concatenate()([x1, xa])\n\n    x = Conv2D(128, 3, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Conv2D(128, 3, strides=2, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    xb = Conv2D(64, 3, strides=2, padding='same')(w3)\n    xb = BatchNormalization()(xb)\n    xb = Activation('relu')(xb)\n    xb = Conv2D(128, 3, padding='same')(xb)\n    xb = BatchNormalization()(xb)\n    xb = Activation('relu')(xb)\n\n    x = Concatenate()([x, xb])\n    x = Conv2D(256, 3, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Conv2D(256, 3, strides=2, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    xc = Conv2D(64, 3, strides=2, padding='same')(w4)\n    xc = BatchNormalization()(xc)\n    xc = Activation('relu')(xc)\n    xc = Conv2D(256, 3, padding='same')(xc)\n    xc = BatchNormalization()(xc)\n    xc = Activation('relu')(xc)\n    xc = Conv2D(256, 3, padding='same')(xc)\n    xc = BatchNormalization()(xc)\n    xc = Activation('relu')(xc)\n\n    x = Concatenate()([x, xc])\n    x = Conv2D(256, 3, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Conv2D(256, 3, strides=2, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    shape = tf.keras.backend.int_shape(x)\n    x = Reshape((shape[1]*shape[2], shape[3]))(x)\n\n    if use_checkpointing:\n        x = CheckpointedTransformerBlock(num_heads=4, ff_dim=128)(x)\n    else:\n        x = transformer_block(x, num_heads=4, ff_dim=128)\n\n    x = GlobalAveragePooling1D()(x)\n    x = Dense(512, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n    x = Dense(128, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    out = Dense(1, activation='sigmoid', dtype='float32')(x)\n\n    model = Model(inputs=inp, outputs=out)\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n\ndef evaluate_model(model, val_gen, title):\n    val_gen.reset()\n    preds = model.predict(val_gen, verbose=0)\n    y_true = val_gen.classes\n    y_pred = (preds > 0.5).astype(int).reshape(-1)\n    y_prob = preds.reshape(-1)\n\n    print(f\"\\nðŸ“‹ Classification Report for {title}:\\n\")\n    print(classification_report(y_true, y_pred, digits=4))\n\n    acc = accuracy_score(y_true, y_pred)\n    prec = precision_score(y_true, y_pred)\n    rec = recall_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    cm = confusion_matrix(y_true, y_pred)\n    tn, fp, fn, tp = cm.ravel()\n    spec = tn / (tn + fp)\n    fpr, tpr, _ = roc_curve(y_true, y_prob)\n    roc_auc = auc(fpr, tpr)\n\n    print(f\"âœ… Overall Metrics for {title}:\")\n    print(f\"Accuracy:     {acc:.4f}\")\n    print(f\"Precision:    {prec:.4f}\")\n    print(f\"Recall:       {rec:.4f}\")\n    print(f\"F1 Score:     {f1:.4f}\")\n    print(f\"Specificity:  {spec:.4f}\")\n    print(f\"AUC:          {roc_auc:.4f}\")\n\n    plt.figure()\n    plt.plot(fpr, tpr, label=f'{title} (AUC = {roc_auc:.4f})')\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(f\"ROC Curve - {title}\")\n    plt.legend(loc=\"lower right\")\n    plt.grid(True)\n    plt.show()\n\n\ndef train_and_measure_memory(use_checkpointing=False, use_full_wavelet=False):\n    tf.keras.backend.clear_session()\n    model = build_model(use_checkpointing=use_checkpointing, use_full_wavelet=use_full_wavelet)\n\n    gpu_available = tf.config.list_physical_devices('GPU')\n    if gpu_available:\n        tf.config.experimental.reset_memory_stats('GPU:0')\n\n    model.fit(\n        train_gen,\n        validation_data=val_gen,\n        epochs=EPOCHS,\n        steps_per_epoch=len(train_gen),\n        validation_steps=len(val_gen),\n        verbose=1\n    )\n\n    title = f\"{'Checkpointing' if use_checkpointing else 'No Checkpointing'} | {'Full Wavelet' if use_full_wavelet else 'Simulated Wavelet'}\"\n    evaluate_model(model, val_gen, title)\n\n    if gpu_available:\n        gpu_mem = tf.config.experimental.get_memory_info('GPU:0')\n        peak_bytes = gpu_mem['peak'] if 'peak' in gpu_mem else gpu_mem['current']\n        return peak_bytes\n    return 0\n\n\nprint(\"ðŸš€ Training WITHOUT checkpointing and simulated wavelet...\")\npeak_mem_no_ckpt = train_and_measure_memory(use_checkpointing=False, use_full_wavelet=False)\nif peak_mem_no_ckpt > 0:\n    print(f\"ðŸ’¾ Peak GPU memory usage: {peak_mem_no_ckpt / 1e6:.2f} MB\")\n\nprint(\"\\nðŸš€ Training WITH checkpointing and full wavelet...\")\npeak_mem_ckpt = train_and_measure_memory(use_checkpointing=True, use_full_wavelet=True)\nif peak_mem_ckpt > 0:\n    print(f\"ðŸ’¾ Peak GPU memory usage: {peak_mem_ckpt / 1e6:.2f} MB\")\n\nif peak_mem_no_ckpt > 0 and peak_mem_ckpt > 0:\n    saved = peak_mem_no_ckpt - peak_mem_ckpt\n    saved_percent = (saved / peak_mem_no_ckpt) * 100\n    print(f\"\\nâœ… Memory saved by gradient checkpointing: {saved / 1e6:.2f} MB ({saved_percent:.2f}%)\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nfrom tensorflow.keras.layers import Layer, Conv2D, Concatenate, Multiply\n\nIMAGE_SIZE = (160, 160)\n\n\ndef WaveletTransformAxisY(input_array):\n    even_indices = input_array[:, ::2]\n    odd_indices = input_array[:, 1::2]\n    low_frequency = (even_indices + odd_indices) / 2\n    high_frequency = (even_indices - odd_indices) / 2\n    return low_frequency, high_frequency\n\ndef WaveletTransformAxisX(input_array):\n    even_indices = input_array[::2, :]\n    odd_indices = input_array[1::2, :]\n    low_frequency = (even_indices + odd_indices) / 2\n    high_frequency = (even_indices - odd_indices) / 2\n    return low_frequency, high_frequency\n\ndef wavelet_decompose_image(image_tensor):\n    # Red channel only for visualization\n    channel = image_tensor[:, :, 0]\n    low, high = WaveletTransformAxisY(channel)\n    low_low, low_high = WaveletTransformAxisX(low)\n    high_low, high_high = WaveletTransformAxisX(high)\n    return [low_low, low_high, high_low, high_high]\n\n\nclass SpatialAttention(Layer):\n    def __init__(self):\n        super(SpatialAttention, self).__init__()\n        self.conv2d = Conv2D(filters=1, kernel_size=7, strides=1,\n                             padding='same', activation='sigmoid', use_bias=False)\n\n    def call(self, input_feature):\n        avg_pool = tf.reduce_mean(input_feature, axis=-1, keepdims=True)\n        max_pool = tf.reduce_max(input_feature, axis=-1, keepdims=True)\n        concat = Concatenate(axis=-1)([avg_pool, max_pool])\n        attention = self.conv2d(concat)\n        return Multiply()([input_feature, attention])\n\n\ndef load_and_preprocess_image(image_path, target_size=IMAGE_SIZE):\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = cv2.resize(image, target_size)\n    image = image / 255.0\n    return image\n\ndef normalize_component(component_array):\n    min_val = component_array.min()\n    max_val = component_array.max()\n    return (component_array - min_val) / (max_val - min_val + 1e-8)\n\n\ndef visualize_wavelet_subbands_with_attention(image_path):\n    image = load_and_preprocess_image(image_path)\n    image_tensor = tf.convert_to_tensor(image, dtype=tf.float32)\n\n    decomposed = wavelet_decompose_image(image_tensor)\n    normalized = [normalize_component(comp.numpy()) for comp in decomposed]\n\n    stacked = np.stack(normalized, axis=-1)\n    stacked_tensor = tf.expand_dims(tf.convert_to_tensor(stacked, dtype=tf.float32), axis=0)\n\n    sa = SpatialAttention()\n    att_output = sa(stacked_tensor)\n    attention_map = tf.reduce_mean(att_output, axis=-1)[0].numpy()\n\n    titles = ['Low-Low (Approximation)',\n              'Low-High (Horizontal Details)',\n              'High-Low (Vertical Details)',\n              'High-High (Diagonal Details)',\n              'Spatial Attention Map']\n\n    plt.figure(figsize=(15, 4))\n    for i, comp in enumerate(normalized):\n        plt.subplot(1, 5, i + 1)\n        plt.imshow(comp, cmap='gray')\n        plt.title(titles[i])\n        plt.axis('off')\n\n    plt.subplot(1, 5, 5)\n    plt.imshow(attention_map, cmap='viridis')\n    plt.title(titles[4])\n    plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n\nvisualize_wavelet_subbands_with_attention(\n    \"/kaggle/input/cell-images-2/cell_images/Parasitized/C100P61ThinF_IMG_20150918_144104_cell_162.png\"\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}